# CS224n-study 2회

[👈 README.md로 되돌아가기](../README.md)

2019년 04일 14일 일요일에 진행한 CS224n 스터디 내용이며, 3강 ([Word Window Classification, Neural Networks, and Matrix Calculus](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture03-neuralnets.pdf)), 4강([Backpropagation and Computation Graphs](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture04-backprop.pdf))을 공부한 내용입니다.

## 내용 정리

### 3강 정리

**자료 링크**

* [영민님 정리](https://baekyeongmin.github.io/cs224n/cs224n_lecture3/)

**질문**

* 진형님 질문: relu가 먼저 나올 것 같았는데 생각외로 아니었네요! -> 원래 sigmoid랑 tanh를 많이 사용했는데, CNN 논문이 나왔을 때 기존과 가장 큰 차이점을 보인 것이 ReLU를 activation function으로 사용한 것이었다. ReLU가 Vanishing Gradient 문제가 없다!! ReLU가 발견되면서 크게 성장한 것 같다!
* 진형님 질문: cross entropy loss를 많이 사용하는 이유 -> 자세한 것은 정리되어있는 좋은 글을 찾아보아야 할 것 같다.
* 욱재님 질문: nlp에서 tanh를 많이 쓰는 이유 -> tanh는 음수가 나올 수 있다. sigmoid는 그래서 잘 쓰지 않는 것 같다.

**참고 링크**

* cross entropy
  * http://funmv2013.blogspot.com/2017/01/cross-entropy.html
  * http://blog.naver.com/PostView.nhn?blogId=gyrbsdl18&logNo=221013188633

### 4강 정리

**자료 링크**

* [욱재님 정리](https://jeongukjae.github.io/posts/cs224n-lecture-4-back-propagation/)
* [영민님 정리](https://baekyeongmin.github.io/cs224n/cs224n_lecture4/)

**보충**

* shape convention -> matrix, layer의 shape를 계산해서 정상적인지 아닌지 확인해보라
* pretrained된 것을 사용했을 때, 동시에 학습이 진행되게 되면 다시 학습이 되니까 이동이 생겨서 학습이 안되는 문제가 발생하는 것이 아닐까요? random initialization인 경우가 아닌 것 같습니다.
* optimizer를 잘 선택하는 것은 너무 상황에 따라 다르다.

**참고 링크**

* [Gradient Descent 관련](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)

## 과제/이슈

안해온 사람이 너무 많아서 다음에!!

## 추후의 내용 정리

* 서기 돌아가면서 하기
* 다음주에는 5강, 6강 내용 해오기
* 정리해오기 - 자율적
* 일정은 그대로
