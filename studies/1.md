# CS224n-study 1회

[👈 README.md로 되돌아가기](../README.md)

2019년 04일 07일 일요일에 진행한 CS224n 스터디 내용이며, 1강 ([Introduction and Word Vectors](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf)), 2강([Word Vectors 2 and Word Senses](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf))을 공부한 내용입니다.

## 질문 검토

### [1번 이슈](https://github.com/jeongukjae/CS224n-study/issues/1)

> ### word2vec 논문에서의 computational complexity 계산법
>
> 제가 computational complexity를 설명하는 부분에서 어떻게 이런 식이 나오는지에 대해 이해가 부족한 것 같습니다 ㅠㅠ

질문을 올렸지만, 보다보니 질문자가 스스로 답을 찾았습니다!

* https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/
* https://arxiv.org/pdf/1301.3781.pdf

### [2번 이슈](https://github.com/jeongukjae/CS224n-study/issues/2)

> ### RNNLM과 NNLM
>
> word2vec 논문에 RNNLM이 NNLM보다 복잡한 패턴을 "이론적으로" 더 잘 표현한다고 적혀있는데, 왜 그런지 궁금합니다.

RNNLM이 왜 "이론적"으로 더 잘 잡는지에 대한 질문이었고, 그 답은 recurrent특성 때문에 이전 것을 더 잘 표현해서 그렇다. ("이전으로 더 많이 연결될 수록 더 정확해진다"는 말이 나왔지만, 정확하진 않다) 그 검증으로 논문의 결과를 보면 의미에 대한 것은 RNNLM이 더 잘 인식한다.

### [3번 이슈](https://github.com/jeongukjae/CS224n-study/issues/3)

> ### word2vec을 구현할 때 u, v 벡터를 구분해서 만드는 이유

easier optimization이 그 이유이고, 끝나고 둘을 잇거나, 평균을 내는데, 어차피 둘이 비슷한 값이 나오게 된다. 하나만 써도 학습이 가능하지만, 둘을 쓰는게 미세하게 성능이 더 좋다고 한다.

### [4번 이슈](https://github.com/jeongukjae/CS224n-study/issues/4)

> ### Word2Vec 의 학습 과정을 시각화해서 보여주는 사이트
>
> https://ronxin.github.io/wevi/
>
> 이슈로 남기는게 맞을지?는 모르겠지만, 공유차원에서 남깁니다!

다음부터는 sharing의 경우 관련 내용을 정리해와서 간략하게 알려주기로 했다.

### [5번 이슈](https://github.com/jeongukjae/CS224n-study/issues/5)

> ### Glove 수식 유도
>
> Glove objective function의 명확한 이해가 잘 되지 않습니다

GloVe objective function에 bias term을 추가해주면 실제 효과가 있을까? 3.2부터 이해가 안된다! (이는 아직 풀리지 않아서 close 안함)

그리고 GloVe는 영어가 잘되는데, 한국어는 잘 안된다. 그래서 영어권에서 baseline으로 쓴다고 한다.

### [6번 이슈](https://github.com/jeongukjae/CS224n-study/issues/6)

> ### Word2vec/ Glove 각각에 대한 3줄요약
>
> Word2vec
>
> 1. Word의 distributed representation을 얻기 위한 새로운 방법의 제시 (기존의 NNLM, RNNLM)
> 2. Skip-gram/CBOW 두가지 모델을 제시했으며 각각 모두 NNLM/RNNLM보다 Computational complexity가 작고 빠른시간에 학습가능
> 3. 단어의 Syntectic/Symentic relation을 평가하는 새로운 방법(단순 dot product를 통한 similarity가 아닌 vector의 거리연산을 이용)과 test set 제시
>
> Glove
>
> 1. 기존의 word2vec(local window-based method)와 LSA(Global Matrix factorization)의 문제점과 해결책을 제시
> 2. W_i*W_j - log(X_ij)를 최소화 하는 objective function을 설정함으로써, 단어의 co-occurence 정보를 직접적으로 학습(log bilinear regression)
> 3. 결과적으로 대부분의 task에서 기존의 방식(word2vec, SVD based LSA)을 뛰어넘음. (후에 대부분 NLP task word representation의 baseline으로 이용됨)

그리고 추가적으로 데이터셋 공개한 것은 큰의미이지 않았을까?

### 추가로 공유된 링크

* https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html
* https://jalammar.github.io

## 이후

### 분량

CS224n 강의에 소개되어 있는 논문은 옛날 것이어서, 시간이 된다면 최신 논문 찾아오는 건 어떨까? 혹은 관련된 것을 찾아오자. 근데 조금 지금 범위가 힘들다. 하지만 그래도 남는게 있지 않을까요??

2강씩 들어될 범위면 2강씩 그대로 진행을 하고, 1강만 들어도 벅찰 것 같은 그런 강의들은 1강만 진행하도록 합니다.

### 브리핑

각각 강의를 정리해오자! 강제성은 없으니 알아서 정리해오는 것으로 합니다.

* 3강 정리하는 사람: @juungbae, @Baekyeongmin
* 4강 정리하는 사람: @seo-jinBro, @jeongukjae

### 과제

과제는 가능한 사람만 해오도록 하고, 내용 정리한 것을 발표하는 순서와 질문에 대한 순서 사이에 진행을 하자!

순서: 내용 정리 - 과제 정리 - 질문(이슈) 정리
