# CS224n-study 4회

[👈 README.md로 되돌아가기](../README.md)

2019년 04월 21일 일요일에 진행한 CS224n 스터디 내용이며, 7강 ([Vanishing Gradients and Fancy RNNs](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf)), 8강([The probability of a sentence? Recurrent Neural Networks and Language Models](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)을 공부한 내용입니다.

## 내용 정리

### 7강 정리

**자료 링크**

* 추가해 주세요 :smile:

**질문**

* 진형님 질문: **LSTM**에서 Forget, Input, Output Gate에서, 학습을 어떻게 시키나요? -> Backpropagation 똑같이 하는데, 결과적으로 hidden state와 cell state가 나온다. 차이점은 결과값이 2개라는 것 뿐이고, 본질적으로는 같다. 
  * 실제로 느린건 알겠는데, 얼마나 느리나요? => LSTM이 느리다기보다는 RNN 구조상 느릴 수 밖에 없다. (병렬화가 되지 않음) 그래도 CNN이 LSTM보다 훨 빠르긴 하다. 
* 진형님 질문: **Transformer**가 대세라고 한다. 나아중에 나오기는 하는데, 정확히 어떤 것인가요? => Seq2Seq로 넘어가면서 RNN+Attention이었는데, Attention is only need라는 논문에, attention만으로 NLP 문제를 풀려고 한 네트워크. 이게 성능이 다른 것보다 좋았음. 거기서 나온 버트는 Attention의 인코더와 디코더 중 인코더만 떼서 만든 것.  기존 목적에 맞게 모델을 학습시키는 게 아니라 일단 언어를 잘 이해하고 있는 네트워크를 만들고, 그 이후에 테스크를 결정.
  * RNN을 떼어놓고 attention만을 한다는 게 이해가 잘 안되는데.. => RNN을 하는 이유가 특정 context 정보를 얻기 위함이다 (특히 순서). Attention도 관련 정보를 넣어서 결과를 얻어 낼 수 있다. 

**참고 링크**

* LSTM, RNN [추가자료](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/) 
* 소셜로 쓰는 딥 러닝 [발표자료](https://archive.pycon.kr/2018/program/28)

### 8강 정리

**자료 링크**


**질문**

* 욱재님 질문: Seq2Seq에서 &lt;END&gt; 를 보장할 수 있을까요? => 문장이 완성이 안되는 경우가 발생함. 그런게 MAX Seq Length를 넘어가서 그런 문제가 생긴 겁니다.
* 진형님 질문: N-gram으로 평가하는 게, 너무 좋지 않을 거 같다고 생각하는데 실제로는 어떠나요? => 번역 논문을 보면 블루스코어를 벤치마크 결과로 사용하는데, 사실 n-gram만 봐도 번역의 결과가 좋을 수 있다고 생각.
  * 어순이 fix가 되지 않은 언어면 n-gram이 별로 좋지 않을 듯. 언어 By 언어일듯. 하지만 잘된다는 독일어-영어도 별로 안 좋은듯. 



**참고 링크**

* F8 Session [영상 링크](https://youtu.be/j48PqBP-OA0?t=1595)

  

## 과제/이슈

Assignment 2 에서 image 뽑는 것 그렇게 오래 안걸리니 각자 해보는 것으로.

## 추후의 내용 정리

* 4/28, 5/5 은 스터디 없음
* 다음 스터디까지 7,8강 진행
* 과제는 3, 4까지 진행할 수 있으면 진행 (자율)
